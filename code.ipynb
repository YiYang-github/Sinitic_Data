{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file: **only** involves calculating storage-needed weights, not for plotting\n",
    "- 1. Generating distance matrix from raw transcription (for Yubao), and select parts of dialects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data4_dir, data4_dir_matrix = 'Data4/transcription_areas.pkl', 'Data4/distance_matrices.npz'\n",
    "\n",
    "def list_available_data():\n",
    "    \"\"\"\n",
    "    列出可用的数据集及其包含的特征\n",
    "    \"\"\"\n",
    "    available_data = {\n",
    "        'Data4': {\n",
    "            'description': '中国语言资源保护工程',\n",
    "            'features': [\n",
    "                'word_name', 'area', 'slice', 'slices', 'coords',\n",
    "                'initial', 'final', 'tone',\n",
    "                'initials_distance', 'finals_distance', 'tones_distance', 'overall_distance'\n",
    "            ]\n",
    "        }\n",
    "        # 可以添加其他数据集\n",
    "    }\n",
    "    return available_data\n",
    "\n",
    "def load_feats(name, features=None):\n",
    "    \"\"\"\n",
    "    加载指定数据集的指定特征\n",
    "\n",
    "    Args:\n",
    "        name (str): 数据集名称 (e.g., 'Data4')\n",
    "        features (list, optional): 需要加载的特征列表.\n",
    "                                     如果为None，则加载所有特征. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dict: 包含请求特征的字典\n",
    "    \"\"\"\n",
    "    loaded_data = {}\n",
    "\n",
    "    if name == 'Data4':\n",
    "        all_features = [\n",
    "            'word_name', 'area', 'slice', 'slices', 'coords',\n",
    "            'initial', 'final', 'tone',\n",
    "            'initials_distance', 'finals_distance', 'tones_distance', 'overall_distance'\n",
    "        ]\n",
    "        if features is None:\n",
    "            features_to_load = all_features\n",
    "        else:\n",
    "            features_to_load = [f for f in features if f in all_features]\n",
    "            if len(features_to_load) != len(features):\n",
    "                print(f\"Warning: Some requested features for {name} are not available.\")\n",
    "\n",
    "        # 加载 pkl 文件中的数据\n",
    "        pkl_features = ['word_name', 'area', 'slice', 'slices', 'coords', 'initial', 'final', 'tone']\n",
    "        if any(f in features_to_load for f in pkl_features):\n",
    "            try:\n",
    "                with open(data4_dir, 'rb') as f:\n",
    "                    data_dict = pickle.load(f)\n",
    "                if 'initial' in features_to_load:\n",
    "                     loaded_data['initial'] = data_dict.get('initial')\n",
    "                if 'final' in features_to_load:\n",
    "                     loaded_data['final'] = data_dict.get('final')\n",
    "                if 'tone' in features_to_load:\n",
    "                     loaded_data['tone'] = data_dict.get('tone')\n",
    "                if 'word_name' in features_to_load:\n",
    "                    loaded_data['word_name'] = data_dict.get('word_name')\n",
    "                if 'area' in features_to_load:\n",
    "                    loaded_data['area'] = data_dict.get('area')\n",
    "                if 'slice' in features_to_load:\n",
    "                    loaded_data['slice'] = data_dict.get('slice')\n",
    "                if 'slices' in features_to_load:\n",
    "                    loaded_data['slices'] = data_dict.get('slices')\n",
    "                if 'coords' in features_to_load:\n",
    "                    loaded_data['coords'] = data_dict.get('coords')\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Error: {data4_dir} not found.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {data4_dir}: {e}\")\n",
    "\n",
    "\n",
    "        # 加载 npz 文件中的数据\n",
    "        npz_features = ['initials_distance', 'finals_distance', 'tones_distance', 'overall_distance']\n",
    "        if any(f in features_to_load for f in npz_features):\n",
    "            try:\n",
    "                loaded_npz = np.load(data4_dir_matrix)\n",
    "                if 'initials_distance' in features_to_load:\n",
    "                    loaded_data['initials_distance'] = loaded_npz.get('initials')\n",
    "                if 'finals_distance' in features_to_load:\n",
    "                    loaded_data['finals_distance'] = loaded_npz.get('finals')\n",
    "                if 'tones_distance' in features_to_load:\n",
    "                    loaded_data['tones_distance'] = loaded_npz.get('tones')\n",
    "                if 'overall_distance' in features_to_load:\n",
    "                     loaded_data['overall_distance'] = loaded_npz.get('overall')\n",
    "                loaded_npz.close() # Close the npz file\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Error: {data4_dir_matrix} not found.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {data4_dir_matrix}: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: Dataset '{name}' not found.\")\n",
    "\n",
    "    return loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os # 导入 os 库用于路径拼接\n",
    "\n",
    "# --- 数据文件路径定义 (请根据你的实际文件位置修改这些路径) ---\n",
    "# 假设你的数据文件存放在项目根目录下的 data/Data4 文件夹内\n",
    "# 你可能需要根据实际情况调整这些路径\n",
    "BASE_DATA_DIR = 'Data4' # 数据文件所在的基准目录\n",
    "\n",
    "# Data4 的原始转写和元数据 pkl 文件\n",
    "data4_raw_data_path = os.path.join(BASE_DATA_DIR, 'transcription_areas.pkl') \n",
    "data4_distance_matrix_path = os.path.join(BASE_DATA_DIR, 'distance_matrices.npz')\n",
    "data4_processed_info_path = os.path.join(BASE_DATA_DIR, 'processed_info.pkl')\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "\n",
    "def load_feats(name, type=None, features=None):\n",
    "    \"\"\"\n",
    "    加载指定数据集的指定类型或指定特征的数据。\n",
    "\n",
    "    Args:\n",
    "        name (str): 数据集名称 (e.g., 'Data4')\n",
    "        type (str, optional): 需要加载的数据类型 (e.g., 'raw', 'distance_matrices').\n",
    "                              如果指定 type，函数会加载该类型下的预定义特征。\n",
    "                              如果 type 为 None，则必须通过 features 参数指定要加载的特征。\n",
    "        features (list, optional): 需要加载的特征列表.\n",
    "                                     如果 type 已指定，features 可用于过滤该类型下的特征。\n",
    "                                     如果 type 为 None，则加载此列表中指定的特征。\n",
    "                                     Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dict: 包含请求特征的字典，键为特征名，值为对应的数据。\n",
    "              如果加载失败或未找到数据集/类型，返回空字典或 None。\n",
    "    \"\"\"\n",
    "    loaded_data = {}\n",
    "\n",
    "    # --- 定义不同数据集和类型下的特征列表和文件路径 ---\n",
    "    # 这个字典定义了每个数据集名称下，不同类型对应哪些预定义特征以及从哪个文件加载\n",
    "    DATASET_CONFIG = {\n",
    "        'Data4': {\n",
    "            'raw': {\n",
    "                'file': data4_raw_data_path,\n",
    "                'pkl_keys': ['word_name', 'area', 'slice', 'slices', 'coords', 'initial', 'final', 'tone'],\n",
    "                'loader': 'pickle' # 指定加载方式\n",
    "            },\n",
    "            'distance_matrices': {\n",
    "                'file': data4_distance_matrix_path,\n",
    "                'npz_keys': ['initials', 'finals', 'tones', 'overall'], # npz 文件中的键名\n",
    "                'output_keys': ['initials_distance', 'finals_distance', 'tones_distance', 'overall_distance'], # 输出字典中的键名\n",
    "                'loader': 'numpy_npz' # 指定加载方式\n",
    "            },\n",
    "            'info': {\n",
    "                'file': data4_processed_info_path,\n",
    "                # 处理后信息文件的键名和输出键名一致\n",
    "                'pkl_keys': ['areas', 'slice', 'slices', 'coords', 'word_names'], # 注意这里的键名与保存时字典的键名对应\n",
    "                'loader': 'pickle'}\n",
    "            # 可以继续添加其他 type...\n",
    "            # 'another_type': {...}\n",
    "        }\n",
    "        # 可以继续添加其他 name...\n",
    "        # 'AnotherDataset': {...}\n",
    "    }\n",
    "\n",
    "    # --- 检查数据集名称是否存在 ---\n",
    "    if name not in DATASET_CONFIG:\n",
    "        print(f\"错误: 数据集 '{name}' 的配置不存在。\")\n",
    "        return {} # 返回空字典表示失败\n",
    "\n",
    "    dataset_config = DATASET_CONFIG[name]\n",
    "\n",
    "    # --- 根据 type 或 features 确定要加载的特征和文件 ---\n",
    "    features_to_load_final = [] # 最终确定要加载的特征列表\n",
    "    file_to_load = None\n",
    "    loader_type = None\n",
    "    source_keys = {} # 记录源文件中的键名到目标输出键名的映射\n",
    "\n",
    "    if type:\n",
    "        if type not in dataset_config:\n",
    "            print(f\"错误: 数据集 '{name}' 不支持类型 '{type}'。\")\n",
    "            return {}\n",
    "\n",
    "        type_config = dataset_config[type]\n",
    "        file_to_load = type_config.get('file')\n",
    "        loader_type = type_config.get('loader')\n",
    "\n",
    "        if loader_type == 'pickle':\n",
    "            all_type_features = type_config.get('pkl_keys', [])\n",
    "            # pickle 加载是直接从字典取，源键和输出键一致\n",
    "            source_keys = {k: k for k in all_type_features}\n",
    "        elif loader_type == 'numpy_npz':\n",
    "            all_type_features = type_config.get('output_keys', [])\n",
    "            # npz 加载需要处理源键到输出键的映射\n",
    "            npz_keys = type_config.get('npz_keys', [])\n",
    "            output_keys = type_config.get('output_keys', [])\n",
    "            if len(npz_keys) == len(output_keys):\n",
    "                 source_keys = dict(zip(output_keys, npz_keys)) # 存储 输出键 -> 源键 映射\n",
    "            else:\n",
    "                 print(f\"配置错误: 数据集 '{name}', 类型 '{type}' 的 npz_keys 和 output_keys 数量不匹配。\")\n",
    "                 return {}\n",
    "        else:\n",
    "             print(f\"配置错误: 数据集 '{name}', 类型 '{type}' 指定了未知的 loader '{loader_type}'。\")\n",
    "             return {}\n",
    "\n",
    "\n",
    "        if features is None:\n",
    "            # 如果没有指定 features，则加载该 type 下的所有预定义特征\n",
    "            features_to_load_final = all_type_features\n",
    "        else:\n",
    "            # 如果指定了 features，则加载该 type 下 features 中包含的特征\n",
    "            features_to_load_final = [f for f in features if f in all_type_features]\n",
    "            if len(features_to_load_final) != len(features):\n",
    "                # 检查用户请求的 features 中是否有不属于该 type 的\n",
    "                not_available = [f for f in features if f not in all_type_features]\n",
    "                print(f\"警告: 请求的特征 {not_available} 不属于数据集 '{name}' 的类型 '{type}'，将被忽略。\")\n",
    "\n",
    "    elif features is not None:\n",
    "         # 如果 type 为 None 但指定了 features (旧的使用方式，可以保留兼容性或弃用)\n",
    "         # 为了新设计清晰，建议要求必须指定 type\n",
    "         print(\"错误: 未指定数据加载类型 (type)，请指定如 type='raw'。\")\n",
    "         return {}\n",
    "         # 以下是保留旧 features 用法的代码，如果需要兼容可以启用：\n",
    "         # print(\"警告: 未指定数据类型 (type)，尝试按特征列表加载 (旧模式)。\")\n",
    "         # # 需要遍历所有 type 才能找到哪些特征在哪里，比较复杂，不推荐。\n",
    "         # # 更好的方式是：要求用户必须指定 type。\n",
    "         # pass # 在新设计中不推荐无 type 加载\n",
    "\n",
    "    else:\n",
    "        # type 和 features 都为 None\n",
    "        print(\"错误: 既未指定数据加载类型 (type)，也未指定要加载的特征列表 (features)。\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "    # --- 执行数据加载 ---\n",
    "    if not file_to_load:\n",
    "         print(\"内部错误: 未能确定要加载的文件路径。\")\n",
    "         return {}\n",
    "\n",
    "    if not features_to_load_final:\n",
    "        print(f\"未找到需要加载的特征列表，请检查 type 或 features 参数。\")\n",
    "        return {}\n",
    "\n",
    "    print(f\"正在从文件 '{file_to_load}' 加载数据...\")\n",
    "    print(f\"计划加载的特征: {features_to_load_final}\")\n",
    "\n",
    "    try:\n",
    "        if loader_type == 'pickle':\n",
    "            with open(file_to_load, 'rb') as f:\n",
    "                data_dict = pickle.load(f)\n",
    "\n",
    "            for feature_name in features_to_load_final:\n",
    "                # 从加载的字典中按特征名获取数据\n",
    "                # 使用 .get() 避免 KeyError 如果文件中的确缺少该特征\n",
    "                if feature_name in data_dict:\n",
    "                     loaded_data[feature_name] = data_dict[feature_name]\n",
    "                else:\n",
    "                     print(f\"警告: 在文件 '{file_to_load}' 中未找到特征 '{feature_name}'。\")\n",
    "\n",
    "        elif loader_type == 'numpy_npz':\n",
    "            loaded_npz = np.load(file_to_load)\n",
    "            for output_key in features_to_load_final:\n",
    "                 source_key = source_keys.get(output_key) # 获取 npz 文件中的对应键\n",
    "                 if source_key and source_key in loaded_npz:\n",
    "                     loaded_data[output_key] = loaded_npz[source_key]\n",
    "                 else:\n",
    "                     print(f\"警告: 在文件 '{file_to_load}' 中未找到特征 '{output_key}' (查找键 '{source_key}')。\")\n",
    "            loaded_npz.close() # 关闭 npz 文件句柄\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误: 数据文件 '{file_to_load}' 未找到。请检查路径设置。\")\n",
    "        return {} # 返回空字典表示失败\n",
    "    except Exception as e:\n",
    "        print(f\"错误加载文件 '{file_to_load}': {e}\")\n",
    "        return {} # 返回空字典表示失败\n",
    "\n",
    "    print(f\"成功加载 {len(loaded_data)} 个特征。\")\n",
    "    return loaded_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从文件 'Data4/transcription_areas.pkl' 加载数据...\n",
      "计划加载的特征: ['word_name', 'area', 'slice', 'slices', 'coords', 'initial', 'final', 'tone']\n",
      "成功加载 8 个特征。\n"
     ]
    }
   ],
   "source": [
    "dict_ = load_feats(name='Data4', type='raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = load_feats(name='Data4', type='raw')\n",
    "# ['initial', 'final', 'tone', 'word_name', 'area', 'slice', 'slices', 'coords']\n",
    "initials, finals, tones, word_names, areas, slice, slices, coords = dict['initial'], dict['final'], dict['tone'], dict['word_name'], dict['area'], dict['slice'], dict['slices'], dict['coords']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Calculate and Save Distance Matrix\n",
    "We first calculate the ratio of missing values of different dialects and features, any only select those with non-missing value above 70%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Ratio of Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ratio_missing_transcription(numpy_array, missing_value='MISSING', threshold_percentage=30):\n",
    "    \"\"\"\n",
    "    分析 NumPy 数组中指定缺失值的比例，找出超过阈值的行和列，\n",
    "    并返回缺失比例最高的各自 Top 10。输出结果使用标准 Python 数字类型。\n",
    "\n",
    "    Args:\n",
    "        numpy_array (np.ndarray): 输入的 NumPy 数组，应为二维。\n",
    "        missing_value (str): 表示缺失值的字符串或值，默认为 'MISSING'。\n",
    "        threshold_percentage (int or float): 缺失值比例阈值 (0-100)，\n",
    "                                            超过此比例的行/列会被单独列出。默认为 30。\n",
    "\n",
    "    Returns:\n",
    "        dict: 包含分析结果的字典，键值如下：\n",
    "            'rows_over_threshold': 缺失比例超过阈值的行索引列表 (Python int)。\n",
    "            'cols_over_threshold': 缺失比例超过阈值的列索引列表 (Python int)。\n",
    "            'top10_rows': 缺失比例最高的 Top 10 行 (索引, 比例) 列表。\n",
    "                           格式为 [(行索引: Python int, 缺失百分比: Python float), ...]。\n",
    "            'top10_cols': 缺失比例最高的 Top 10 列 (索引, 比例) 列表。\n",
    "                           格式为 [(列索引: Python int, 缺失百分比: Python float), ...]。\n",
    "        None: 如果输入不是二维 NumPy 数组，则返回 None 并打印错误信息。\n",
    "    \"\"\"\n",
    "    if not isinstance(numpy_array, np.ndarray) or numpy_array.ndim != 2:\n",
    "        print(\"错误：输入必须是一个二维 NumPy 数组。\")\n",
    "        return None\n",
    "\n",
    "    n_rows, n_cols = numpy_array.shape\n",
    "    print(f\"正在分析一个 {n_rows} 行, {n_cols} 列的数组...\")\n",
    "    print(f\"缺失值标记为: '{missing_value}'\")\n",
    "    print(f\"缺失比例阈值为: {threshold_percentage}%\")\n",
    "\n",
    "    # 创建一个布尔掩码，标记出所有缺失值的位置\n",
    "    missing_mask = (numpy_array == missing_value)\n",
    "\n",
    "    # --- 计算每行的缺失比例 ---\n",
    "    row_missing_counts = np.sum(missing_mask, axis=1)\n",
    "    row_missing_percentages = (row_missing_counts / n_cols) * 100\n",
    "\n",
    "    # --- 计算每列的缺失比例 ---\n",
    "    col_missing_counts = np.sum(missing_mask, axis=0)\n",
    "    col_missing_percentages = (col_missing_counts / n_rows) * 100\n",
    "\n",
    "    print(\"缺失比例计算完成。\")\n",
    "\n",
    "    # --- 找出比例超过阈值的行和列 ---\n",
    "    # 转换为标准 Python int 列表\n",
    "    rows_over_threshold_indices = [int(i) for i in np.where(row_missing_percentages > threshold_percentage)[0]]\n",
    "    cols_over_threshold_indices = [int(i) for i in np.where(col_missing_percentages > threshold_percentage)[0]]\n",
    "\n",
    "    print(f\"发现 {len(rows_over_threshold_indices)} 行的缺失比例超过 {threshold_percentage}%。\")\n",
    "    print(f\"发现 {len(cols_over_threshold_indices)} 列的缺失比例超过 {threshold_percentage}%。\")\n",
    "\n",
    "    # --- 找出缺失比例最高的 Top 10 行 ---\n",
    "    top10_row_indices = np.argsort(-row_missing_percentages)[:10]\n",
    "    top10_row_percentages = row_missing_percentages[top10_row_indices]\n",
    "    # 转换为标准 Python int 和 float 元组列表\n",
    "    top10_rows_result = [(int(idx), float(round(pct, 2))) for idx, pct in zip(top10_row_indices, top10_row_percentages)]\n",
    "\n",
    "    # --- 找出缺失比例最高的 Top 10 列 ---\n",
    "    top10_col_indices = np.argsort(-col_missing_percentages)[:10]\n",
    "    top10_col_percentages = col_missing_percentages[top10_col_indices]\n",
    "    # 转换为标准 Python int 和 float 元组列表\n",
    "    top10_cols_result = [(int(idx), float(round(pct, 2))) for idx, pct in zip(top10_col_indices, top10_col_percentages)]\n",
    "\n",
    "    print(\"Top 10 缺失比例行/列计算完成。\")\n",
    "\n",
    "    # --- 构建结果字典 ---\n",
    "    results = {\n",
    "        'rows_over_threshold': rows_over_threshold_indices,\n",
    "        'cols_over_threshold': cols_over_threshold_indices,\n",
    "        'top10_rows': top10_rows_result,\n",
    "        'top10_cols': top10_cols_result,\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在分析一个 1289 行, 999 列的数组...\n",
      "缺失值标记为: 'MISSING'\n",
      "缺失比例阈值为: 20%\n",
      "缺失比例计算完成。\n",
      "发现 6 行的缺失比例超过 20%。\n",
      "发现 2 列的缺失比例超过 20%。\n",
      "Top 10 缺失比例行/列计算完成。\n"
     ]
    }
   ],
   "source": [
    "results_initials = ratio_missing_transcription(initials, missing_value='MISSING', threshold_percentage=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在分析一个 1289 行, 999 列的数组...\n",
      "缺失值标记为: 'MISSING'\n",
      "缺失比例阈值为: 20%\n",
      "缺失比例计算完成。\n",
      "发现 186 行的缺失比例超过 20%。\n",
      "发现 84 列的缺失比例超过 20%。\n",
      "Top 10 缺失比例行/列计算完成。\n"
     ]
    }
   ],
   "source": [
    "results_finals = ratio_missing_transcription(finals, missing_value='MISSING', threshold_percentage=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在分析一个 1289 行, 999 列的数组...\n",
      "缺失值标记为: 'MISSING'\n",
      "缺失比例阈值为: 20%\n",
      "缺失比例计算完成。\n",
      "发现 23 行的缺失比例超过 20%。\n",
      "发现 2 列的缺失比例超过 20%。\n",
      "Top 10 缺失比例行/列计算完成。\n"
     ]
    }
   ],
   "source": [
    "results_tones = ratio_missing_transcription(tones, missing_value='MISSING', threshold_percentage=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205 [7, 520, 524, 14, 1039, 17, 533, 535, 541, 31, 550, 551, 40, 41, 553, 555, 556, 559, 561, 562, 563, 564, 565, 568, 574, 575, 1087, 580, 583, 585, 586, 587, 76, 588, 81, 1105, 83, 595, 596, 87, 599, 604, 1117, 94, 606, 1124, 101, 104, 108, 111, 112, 113, 114, 115, 1138, 118, 1144, 121, 1147, 125, 131, 132, 644, 134, 1155, 138, 141, 144, 145, 146, 656, 148, 149, 150, 151, 153, 1178, 669, 158, 159, 1185, 673, 165, 677, 168, 1193, 170, 171, 172, 174, 1199, 176, 1200, 178, 692, 1205, 1211, 187, 189, 1213, 192, 705, 196, 197, 710, 1221, 1224, 1226, 204, 1230, 719, 1231, 724, 734, 1248, 1250, 1251, 1259, 1263, 1264, 1273, 1274, 254, 1282, 259, 1284, 1286, 1288, 277, 286, 288, 801, 296, 331, 848, 850, 853, 345, 347, 859, 864, 865, 873, 878, 372, 886, 890, 378, 380, 386, 387, 904, 397, 398, 911, 913, 405, 408, 409, 412, 928, 932, 422, 424, 425, 427, 428, 431, 432, 434, 439, 441, 958, 961, 450, 963, 964, 965, 969, 459, 460, 971, 974, 972, 973, 976, 982, 983, 473, 1016, 988, 476, 477, 479, 480, 992, 996, 1003, 494, 1008, 1012, 1013, 500, 504, 511]\n",
      "84 [26, 562, 565, 566, 567, 571, 572, 576, 577, 578, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 600, 601, 607, 608, 609, 610, 611, 615, 616, 618, 619, 620, 669, 670, 672, 673, 677, 683, 684, 685, 686, 689, 696, 697, 698, 699, 700, 203, 220, 221, 222, 749, 750, 239, 751, 752, 753, 754, 755, 756, 770, 828, 365, 366, 368, 369, 371, 907, 912, 401, 970, 971, 972, 477, 994, 995, 996, 997, 998]\n"
     ]
    }
   ],
   "source": [
    "# first operate dialects \n",
    "all_row_indices = results_initials['rows_over_threshold'] + results_finals['rows_over_threshold'] + results_tones['rows_over_threshold']\n",
    "unique_row_indices_set = set(all_row_indices)\n",
    "unique_row_indices_list = list(unique_row_indices_set)\n",
    "print(len(unique_row_indices_list), unique_row_indices_list)\n",
    "\n",
    "all_col_indices = results_initials['cols_over_threshold'] + results_finals['cols_over_threshold'] + results_tones['cols_over_threshold']\n",
    "unique_col_indices_set = set(all_col_indices)\n",
    "unique_col_indices_list = list(unique_col_indices_set)\n",
    "print(len(unique_col_indices_list), unique_col_indices_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def remove_rows_and_cols(numpy_array, rows_to_delete, cols_to_delete):\n",
    "    \"\"\"\n",
    "    从一个二维 NumPy 数组中删除指定的行和列。\n",
    "\n",
    "    Args:\n",
    "        numpy_array (np.ndarray): 输入的二维 NumPy 数组。\n",
    "        rows_to_delete (list or array_like): 要删除的行的索引列表或数组。\n",
    "        cols_to_delete (list or array_like): 要删除的列的索引列表或数组。\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 删除指定行和列后的新的 NumPy 数组。\n",
    "        None: 如果输入不是二维 NumPy 数组，则返回 None 并打印错误信息。\n",
    "    \"\"\"\n",
    "    if not isinstance(numpy_array, np.ndarray) or numpy_array.ndim != 2:\n",
    "        print(\"错误：输入必须是一个二维 NumPy 数组。\")\n",
    "        return None\n",
    "\n",
    "    print(f\"原始数组维度: {numpy_array.shape}\")\n",
    "    print(f\"将删除 {len(rows_to_delete)} 行和 {len(cols_to_delete)} 列...\")\n",
    "\n",
    "    # 使用 np.delete 删除行\n",
    "    # axis=0 表示删除行\n",
    "    # 返回一个新的数组，原始数组不变\n",
    "    array_after_rows_deleted = np.delete(numpy_array, rows_to_delete, axis=0)\n",
    "\n",
    "    # 使用 np.delete 删除列\n",
    "    # 注意：这里是在删除行后的新数组上操作\n",
    "    # axis=1 表示删除列\n",
    "    array_after_cols_deleted = np.delete(array_after_rows_deleted, cols_to_delete, axis=1)\n",
    "\n",
    "    print(f\"删除后的数组维度: {array_after_cols_deleted.shape}\")\n",
    "\n",
    "    return array_after_cols_deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数组维度: (1289, 999)\n",
      "将删除 205 行和 84 列...\n",
      "删除后的数组维度: (1084, 915)\n",
      "原始数组维度: (1289, 999)\n",
      "将删除 205 行和 84 列...\n",
      "删除后的数组维度: (1084, 915)\n",
      "原始数组维度: (1289, 999)\n",
      "将删除 205 行和 84 列...\n",
      "删除后的数组维度: (1084, 915)\n"
     ]
    }
   ],
   "source": [
    "processed_initials = remove_rows_and_cols(initials, unique_row_indices_list, unique_col_indices_list)\n",
    "processed_finals = remove_rows_and_cols(finals, unique_row_indices_list, unique_col_indices_list)\n",
    "processed_tones = remove_rows_and_cols(tones, unique_row_indices_list, unique_col_indices_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Distance Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Optional: Import tqdm for a progress bar, useful for large datasets\n",
    "# from tqdm.auto import tqdm # pip install tqdm\n",
    "\n",
    "def cal_distance(features, missing_value='MISSING'):\n",
    "    \"\"\"\n",
    "    Calculate the distance matrix between dialects based on feature differences.\n",
    "\n",
    "    The distance d(i, j) is the number of features where dialects i and j differ,\n",
    "    divided by the number of features where *neither* dialect i nor j has a\n",
    "    missing value.\n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray): 2D array [n_dialects, n_features], dtype=object.\n",
    "                               Contains feature values (usually strings).\n",
    "        missing_value (str): Indicator of missing values (e.g., 'MISSING', 'Ǿ').\n",
    "                               Features with this value are ignored in pairwise comparisons.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Distance matrix [n_dialects, n_dialects], dtype=float.\n",
    "                    dist_matrix[i, j] is the calculated distance.\n",
    "                    Returns np.nan if a pair of dialects has no comparable features\n",
    "                    (i.e., for every feature, at least one has a missing value).\n",
    "    \"\"\"\n",
    "    if not isinstance(features, np.ndarray) or features.ndim != 2:\n",
    "        raise ValueError(\"Input 'features' must be a 2D NumPy array.\")\n",
    "\n",
    "    n_dialects, n_features = features.shape\n",
    "    print(f\"Calculating distance matrix for {n_dialects} dialects and {n_features} features...\")\n",
    "    print(f\"Using '{missing_value}' as the missing value indicator.\")\n",
    "\n",
    "    # Initialize distance matrix - using np.nan allows easy identification\n",
    "    # of pairs with no comparable features. Diagonal will be set to 0.\n",
    "    dist_matrix = np.full((n_dialects, n_dialects), np.nan, dtype=float)\n",
    "\n",
    "    # Pre-calculate a boolean mask indicating where features are *not* missing\n",
    "    # This avoids repeated comparisons with missing_value inside the loops\n",
    "    is_valid = (features != missing_value)\n",
    "    print(\"Pre-calculated validity mask.\")\n",
    "\n",
    "    # --- Iterate through all unique pairs of dialects (i, j) ---\n",
    "    # Using tqdm here provides a nice progress bar: range(n_dialects) -> tqdm(range(n_dialects))\n",
    "    # for i in tqdm(range(n_dialects), desc=\"Calculating distances\"):\n",
    "    for i in range(n_dialects):\n",
    "        # Optional: Print progress every N dialects\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  Processed {i+1}/{n_dialects} dialects...\")\n",
    "\n",
    "        # Distance from a dialect to itself is 0\n",
    "        dist_matrix[i, i] = 0.0\n",
    "\n",
    "        # Get data and validity mask for dialect i once\n",
    "        features_i = features[i]\n",
    "        is_valid_i = is_valid[i]\n",
    "\n",
    "        # Compare dialect i with dialects j where j > i\n",
    "        for j in range(i + 1, n_dialects):\n",
    "            features_j = features[j]\n",
    "            is_valid_j = is_valid[j]\n",
    "\n",
    "            # 1. Find features where *both* dialects have valid data\n",
    "            #    (element-wise AND)\n",
    "            valid_comparison_mask = is_valid_i & is_valid_j\n",
    "\n",
    "            # 2. Count how many features are valid for this pair\n",
    "            num_valid_features = np.sum(valid_comparison_mask)\n",
    "\n",
    "            # 3. Handle case where there are no features to compare\n",
    "            if num_valid_features == 0:\n",
    "                # Distance is undefined, leave as np.nan\n",
    "                # dist_matrix[i, j] = np.nan # Already initialized to NaN\n",
    "                dist_matrix[j, i] = np.nan # Ensure symmetry for NaN too\n",
    "                continue # Move to the next pair (j)\n",
    "\n",
    "            # 4. Compare feature values *only* where both are valid\n",
    "            #    First, find all differing features between the full rows\n",
    "            feature_differs = (features_i != features_j)\n",
    "            #    Then, select differences only where the comparison was valid\n",
    "            valid_differences_mask = feature_differs & valid_comparison_mask\n",
    "\n",
    "            # 5. Count the number of valid differences\n",
    "            num_differences = np.sum(valid_differences_mask)\n",
    "\n",
    "            # 6. Calculate the distance\n",
    "            distance = num_differences / num_valid_features\n",
    "\n",
    "            # 7. Store the distance (and maintain symmetry)\n",
    "            dist_matrix[i, j] = distance\n",
    "            dist_matrix[j, i] = distance\n",
    "\n",
    "    print(\"Distance matrix calculation finished.\")\n",
    "    return dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initials_distance = cal_distance(processed_initials)\n",
    "finals_distance = cal_distance(processed_finals)\n",
    "tones_distance = cal_distance(processed_tones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功将距离矩阵保存到: Data4/distance_matrices.npz\n"
     ]
    }
   ],
   "source": [
    "overall_distance = (initials_distance + finals_distance + tones_distance) / 3\n",
    "output_filename = 'Data4/distance_matrices.npz'\n",
    "np.savez_compressed(\n",
    "    output_filename,\n",
    "    initials=initials_distance,  # 在 .npz 文件中，这个矩阵被称为 'initials'\n",
    "    finals=finals_distance,    # 这个被称为 'finals'\n",
    "    tones=tones_distance,       # 这个被称为 'tones'\n",
    "    overall=overall_distance  # 如果需要，可以添加整体距离矩阵\n",
    ")\n",
    "print(f\"成功将距离矩阵保存到: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "尝试加载文件: Data4/distance_matrices.npz\n",
      "文件中包含的数组名称: ['initials', 'finals', 'tones', 'overall']\n",
      "加载的 'initials' 矩阵形状: (1084, 1084)\n",
      "加载的 'finals' 矩阵形状: (1084, 1084)\n",
      "加载的 'tones' 矩阵形状: (1084, 1084)\n"
     ]
    }
   ],
   "source": [
    "output_filename = 'Data4/distance_matrices.npz'\n",
    "print(f\"\\n尝试加载文件: {output_filename}\")\n",
    "loaded_data = np.load(output_filename)\n",
    "\n",
    "# 检查文件中有哪些数组\n",
    "print(f\"文件中包含的数组名称: {list(loaded_data.keys())}\")\n",
    "\n",
    "# 加载单个数组\n",
    "loaded_initials = loaded_data['initials']\n",
    "loaded_finals = loaded_data['finals']\n",
    "loaded_tones = loaded_data['tones']\n",
    "overall_distance = loaded_data['overall']\n",
    "\n",
    "print(f\"加载的 'initials' 矩阵形状: {loaded_initials.shape}\")\n",
    "print(f\"加载的 'finals' 矩阵形状: {loaded_finals.shape}\")\n",
    "print(f\"加载的 'tones' 矩阵形状: {loaded_tones.shape}\")\n",
    "\n",
    "# 不要忘记关闭文件（虽然对于 np.load 通常不是严格必需的，但好习惯）\n",
    "loaded_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import typing\n",
    "\n",
    "def get_missing_percentage(numpy_matrix: np.ndarray, missing_value: typing.Any = 'MISSING') -> float:\n",
    "    \"\"\"\n",
    "    计算 NumPy 数组中指定缺失值（默认为 'MISSING'）的元素比例。\n",
    "\n",
    "    Args:\n",
    "        numpy_matrix (np.ndarray): 输入的 NumPy 数组（可以是任意维度）。\n",
    "        missing_value (Any): 用来表示缺失值的值，默认为 'MISSING' 字符串。\n",
    "\n",
    "    Returns:\n",
    "        float: 元素中缺失值的百分比（0.0 到 100.0 之间）。\n",
    "               如果输入数组为空，返回 0.0。\n",
    "               如果输入不是 NumPy 数组，打印错误并返回 0.0。\n",
    "    \"\"\"\n",
    "    if not isinstance(numpy_matrix, np.ndarray):\n",
    "        print(\"错误: 输入必须是一个 NumPy 数组。\")\n",
    "        return 0.0\n",
    "\n",
    "    total_elements = numpy_matrix.size\n",
    "\n",
    "    # 检查数组是否为空，避免除以零\n",
    "    if total_elements == 0:\n",
    "        print(\"警告: 输入数组为空，缺失比例为 0%。\")\n",
    "        return 0.0\n",
    "\n",
    "    # 创建布尔掩码，True 表示元素等于 missing_value\n",
    "    missing_mask = (numpy_matrix == missing_value)\n",
    "\n",
    "    # 统计缺失值的数量 (True 会被当作 1 求和)\n",
    "    missing_count = np.sum(missing_mask)\n",
    "\n",
    "    # 计算并返回百分比\n",
    "    percentage = (missing_count / total_elements) * 100.0\n",
    "    print(f\"缺失值比例: {percentage:.2f}%\")\n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "缺失值比例: 1.01%\n",
      "缺失值比例: 5.98%\n",
      "缺失值比例: 1.22%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(1.2160990462363641)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_missing_percentage(processed_initials, missing_value='MISSING')\n",
    "get_missing_percentage(processed_finals, missing_value='MISSING') \n",
    "get_missing_percentage(processed_tones, missing_value='MISSING')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Processing Other Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import typing\n",
    "\n",
    "def remove_elements_or_rows_by_indices(data: typing.Union[list, np.ndarray], indices_to_delete: typing.List[int]) -> list:\n",
    "    \"\"\"\n",
    "    从列表、一维 NumPy 数组或二维 NumPy 数组中删除指定索引位置的元素/行。\n",
    "\n",
    "    Args:\n",
    "        data (list or np.ndarray): 输入的数据，可以是 Python 列表、一维或二维 NumPy 数组。\n",
    "        indices_to_delete (list): 包含要删除元素的索引（位置）或行的索引的列表（整数）。\n",
    "\n",
    "    Returns:\n",
    "        list: 删除指定元素/行后的结果，统一返回 Python 列表。\n",
    "              对于原始输入是列表或一维 NumPy 数组，返回的是一个扁平的列表。\n",
    "              对于原始输入是二维 NumPy 数组，返回的是一个列表的列表。\n",
    "              如果指定的索引超出范围，超出范围的索引会被忽略。\n",
    "              如果输入数据类型或 NumPy 数组维度不支持，返回 None 并打印错误。\n",
    "    \"\"\"\n",
    "    if not isinstance(indices_to_delete, list):\n",
    "        print(\"错误: 第二个输入（indices_to_delete）必须是 list 类型。\")\n",
    "        return None\n",
    "\n",
    "    # 为了高效查找索引，转换为集合\n",
    "    indices_to_delete_set = set(indices_to_delete)\n",
    "    print(f\"计划删除 {len(indices_to_delete)} 个索引。\")\n",
    "\n",
    "    result_list = None # 初始化为 None，表示尚未成功处理\n",
    "\n",
    "    if isinstance(data, list):\n",
    "        print(\"输入类型: Python 列表\")\n",
    "        print(f\"原始列表长度: {len(data)}\")\n",
    "        # 使用列表推导式创建新列表，只包含索引不在删除集合中的元素\n",
    "        result_list = [item for i, item in enumerate(data) if i not in indices_to_delete_set]\n",
    "        print(f\"删除后列表长度: {len(result_list)}\")\n",
    "\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        print(\"输入类型: NumPy 数组\")\n",
    "        \n",
    "        array_after_deletion = None\n",
    "\n",
    "        if data.ndim == 1:\n",
    "            print(f\"原始 NumPy 数组维度: {data.shape} (一维)\")\n",
    "            # 对于一维数组，np.delete(arr, obj, axis=0) 或 axis=None 都可以删除元素\n",
    "            array_after_deletion = np.delete(data, list(indices_to_delete_set), axis=0)\n",
    "            print(f\"删除后 NumPy 数组维度: {array_after_deletion.shape}\")\n",
    "            # 将一维 NumPy 数组转换为 Python 列表\n",
    "            result_list = array_after_deletion.tolist()\n",
    "\n",
    "        elif data.ndim == 2:\n",
    "            print(f\"原始 NumPy 数组维度: {data.shape} (二维)\")\n",
    "            # 使用 np.delete 按行删除 (axis=0)\n",
    "            array_after_deletion = np.delete(data, list(indices_to_delete_set), axis=0)\n",
    "            print(f\"删除后 NumPy 数组维度: {array_after_deletion.shape}\")\n",
    "            # 将 NumPy 二维数组转换为 Python 列表的列表\n",
    "            result_list = array_after_deletion.tolist()\n",
    "            \n",
    "        else:\n",
    "            print(f\"错误: 输入的 NumPy 数组维度为 {data.ndim}，当前函数只支持一维或二维数组。\")\n",
    "            return None # 返回 None 表示处理失败\n",
    "\n",
    "    else:\n",
    "        print(f\"错误: 不支持的数据类型 '{type(data)}'。输入必须是 list 或 np.ndarray。\")\n",
    "        return None # 返回 None 表示处理失败\n",
    "\n",
    "    return result_list # 返回最终的列表结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计划删除 84 个索引。\n",
      "输入类型: Python 列表\n",
      "原始列表长度: 999\n",
      "删除后列表长度: 915\n"
     ]
    }
   ],
   "source": [
    "removed_word_names = remove_elements_or_rows_by_indices(word_names, unique_col_indices_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计划删除 205 个索引。\n",
      "输入类型: NumPy 数组\n",
      "原始 NumPy 数组维度: (1289,) (一维)\n",
      "删除后 NumPy 数组维度: (1084,)\n",
      "计划删除 205 个索引。\n",
      "输入类型: NumPy 数组\n",
      "原始 NumPy 数组维度: (1289,) (一维)\n",
      "删除后 NumPy 数组维度: (1084,)\n",
      "计划删除 205 个索引。\n",
      "输入类型: NumPy 数组\n",
      "原始 NumPy 数组维度: (1289,) (一维)\n",
      "删除后 NumPy 数组维度: (1084,)\n",
      "计划删除 205 个索引。\n",
      "输入类型: NumPy 数组\n",
      "原始 NumPy 数组维度: (1289, 2) (二维)\n",
      "删除后 NumPy 数组维度: (1084, 2)\n"
     ]
    }
   ],
   "source": [
    "removed_areas, removed_slice, removed_slices, removed_coords = remove_elements_or_rows_by_indices(areas, unique_row_indices_list), remove_elements_or_rows_by_indices(slice, unique_row_indices_list), remove_elements_or_rows_by_indices(slices, unique_row_indices_list), remove_elements_or_rows_by_indices(coords, unique_row_indices_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功将处理后信息保存到 'Data4/processed_info.pkl'\n"
     ]
    }
   ],
   "source": [
    "removed_areas, removed_slice, removed_slices, removed_coords, removed_word_names\n",
    "\n",
    "output_path = 'Data4/processed_info.pkl'\n",
    "data_to_save = {\n",
    "        'areas': removed_areas,\n",
    "        'slice': removed_slice,\n",
    "        'slices': removed_slices,\n",
    "        'coords': removed_coords,\n",
    "        'word_names': removed_word_names\n",
    "    }\n",
    "\n",
    "# 确保目录存在\n",
    "output_dir = os.path.dirname(output_path)\n",
    "if output_dir: # 只有当路径包含目录时才创建\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(data_to_save, f)\n",
    "    print(f\"成功将处理后信息保存到 '{output_path}'\")\n",
    "except Exception as e:\n",
    "    print(f\"错误保存文件 '{output_path}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graduation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
